{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092512bb",
   "metadata": {},
   "source": [
    "### Анализ тональности твитов\n",
    "Создаю переменную tweets, где будут только позитивные и отрицательные сообщения (без имен пользователя и тд)\\\n",
    "Вывожу хвост из 3 данных, чтобы убедиться, что в переменной все твиты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c606e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111920</th>\n",
       "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
       "      <td>@_music_lover_13 оооо, еще и Оля) какая прелес...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111921</th>\n",
       "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
       "      <td>@GazetaRu_All Ныне картина того,как у ЕС,сжало...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111922</th>\n",
       "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
       "      <td>RT @bazzzilio: @VRSoloviev :)) Я все еще жив, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      neg   \n",
       "111920          Вот и в школу, в говно это идти уже надо(  \\\n",
       "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...   \n",
       "111922  Такси везет меня на работу. Раздумываю приплат...   \n",
       "\n",
       "                                                      pos  \n",
       "111920  @_music_lover_13 оооо, еще и Оля) какая прелес...  \n",
       "111921  @GazetaRu_All Ныне картина того,как у ЕС,сжало...  \n",
       "111922  RT @bazzzilio: @VRSoloviev :)) Я все еще жив, ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets_neg = pd.read_csv('negative.csv', delimiter=';', \n",
    "                    names=['none1', 'none2', 'none3', 'text', 'none4', 'none5', \n",
    "                           'none6', 'none7', 'none8', 'none9', 'none10', 'none11'])\n",
    "tweets_pos = pd.read_csv('positive.csv', delimiter=';', \n",
    "                    names=['none1', 'none2', 'none3', 'text', 'none4', 'none5', \n",
    "                           'none6', 'none7', 'none8', 'none9', 'none10', 'none11'])\n",
    "tweets = pd.DataFrame()\n",
    "tweets['neg'], tweets['pos'] = tweets_neg['text'], tweets_pos['text']\n",
    "tweets.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170d6b0",
   "metadata": {},
   "source": [
    "Загрузка русских стоп-слов с помощью библиотеки nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84cacf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d652d6",
   "metadata": {},
   "source": [
    "Обрабатываю сообщения пользователей с помощью регулярных выражений и строки стоп-слов\n",
    "\n",
    "1. Из каждого сообщения оставляю только русские слова и привожу к нижнему регистру\n",
    "2. Делаю из строки массив слов, где разделитель на слова служит любое кол-во пробелов\n",
    "3. Создаю строку так, чтобы проверяемое слово не было стоп-словом и длина была не меньше 2 и нормализую его\n",
    "\n",
    "Функция norm_word:  \n",
    "Для начала проверяю есть ли нужное слово в словаре со словами в начальной форме  \n",
    "Если есть, то возвращаю слово в начальной форму  \n",
    "Если нет, то с помощью библиотеки pymorphy2 и его анализатора, возвращаю слово в нормальной форме и запомнимаю его в словаре.    Возращаю результат  \n",
    "Зачем? Потому что если какждый раз вызывать анализатор pymorphy2, то это будет ОЧЕНЬ долго (я за 5 минут не смог дождаться), так как он каждый раз вызывает свой алгоритм и если одно и тоже слово появляется уже в 10 раз, то и время займет в 10 раз больше. А я буду запоминать значение и если попалось тоже слово, то выводить сразу его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674bb7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уже очищено 10000 из 22000 слов\n",
      "Уже очищено 20000 из 22000 слов\n",
      "Уже очищено 30000 из 22000 слов\n",
      "Уже очищено 40000 из 22000 слов\n",
      "Уже очищено 50000 из 22000 слов\n",
      "Уже очищено 60000 из 22000 слов\n",
      "Уже очищено 70000 из 22000 слов\n",
      "Уже очищено 80000 из 22000 слов\n",
      "Уже очищено 90000 из 22000 слов\n",
      "Уже очищено 100000 из 22000 слов\n",
      "Уже очищено 110000 из 22000 слов\n",
      "Уже очищено 120000 из 22000 слов\n",
      "Уже очищено 130000 из 22000 слов\n",
      "Уже очищено 140000 из 22000 слов\n",
      "Уже очищено 150000 из 22000 слов\n",
      "Уже очищено 160000 из 22000 слов\n",
      "Уже очищено 170000 из 22000 слов\n",
      "Уже очищено 180000 из 22000 слов\n",
      "Уже очищено 190000 из 22000 слов\n",
      "Уже очищено 200000 из 22000 слов\n",
      "Уже очищено 210000 из 22000 слов\n",
      "Уже очищено 220000 из 22000 слов\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>negClean</th>\n",
       "      <th>posClean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111921</th>\n",
       "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
       "      <td>@GazetaRu_All Ныне картина того,как у ЕС,сжало...</td>\n",
       "      <td>тауриэль грусть обнять</td>\n",
       "      <td>ныне картина сжаться страх пытаться убежать ро...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111922</th>\n",
       "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
       "      <td>RT @bazzzilio: @VRSoloviev :)) Я все еще жив, ...</td>\n",
       "      <td>такси везти работа раздумывать приплатить втащ...</td>\n",
       "      <td>живой придурок перестать путать нельсон мандел...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      neg   \n",
       "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...  \\\n",
       "111922  Такси везет меня на работу. Раздумываю приплат...   \n",
       "\n",
       "                                                      pos   \n",
       "111921  @GazetaRu_All Ныне картина того,как у ЕС,сжало...  \\\n",
       "111922  RT @bazzzilio: @VRSoloviev :)) Я все еще жив, ...   \n",
       "\n",
       "                                                 negClean   \n",
       "111921                             тауриэль грусть обнять  \\\n",
       "111922  такси везти работа раздумывать приплатить втащ...   \n",
       "\n",
       "                                                 posClean  \n",
       "111921  ныне картина сжаться страх пытаться убежать ро...  \n",
       "111922  живой придурок перестать путать нельсон мандел...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pymorphy2\n",
    "\n",
    "norm = {}\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def norm_form(word):\n",
    "    res = norm.get(word)\n",
    "    if res:\n",
    "        return res\n",
    "    res = morph.parse(word)[0].normal_form\n",
    "    norm[word] = res\n",
    "    return res\n",
    "\n",
    "\n",
    "def cleaner_tweets(tweets):\n",
    "  k = 1\n",
    "  tempArr = []\n",
    "  for tweet in tweets['neg']:\n",
    "    tweet = re.sub(r'[^а-яА-Я]', ' ', tweet.lower())\n",
    "    tweet = re.split(r'\\s+', tweet)\n",
    "    tweet = ' '.join([norm_form(word) for word in tweet if len(word) > 2 and word not in russian_stopwords])\n",
    "    tempArr.append(tweet)\n",
    "    k += 1\n",
    "    if k % 10000 == 0: print(f\"Уже очищено {k} из 22000 слов\")\n",
    "  tweets['negClean'] = pd.Series(tempArr)\n",
    "  \n",
    "  tempArr = []  \n",
    "  for tweet in tweets['pos']:\n",
    "    tweet = re.sub(r'[^а-яА-Я]', ' ', tweet.lower())\n",
    "    tweet = tweet.split()\n",
    "    tweet = ' '.join([norm_form(word) for word in tweet if len(word) > 2 and word not in russian_stopwords])\n",
    "    tempArr.append(tweet)\n",
    "    k += 1\n",
    "    if k % 10000 == 0: print(f\"Уже очищено {k} из 22000 слов\")\n",
    "  tweets['posClean'] = pd.Series(tempArr)\n",
    "\n",
    "  return tweets\n",
    "    \n",
    "\n",
    "tweets = cleaner_tweets(tweets)\n",
    "tweets.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e8743",
   "metadata": {},
   "source": [
    "Записываю уже очищенные негативные и позитивные твиты в один массив.  \n",
    "Оставляю на них метки, что \"0\" - это негативный твит, а \"1\" - это позитивный твит.  \n",
    "Вывожу полученное слияние  \n",
    "Далее перемешиваю их, чтобы модель смогла обучиться равномерно, иначе первая половина состояла только из отрицательных твитов.  \n",
    "Вывожу полученное перемешивание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f8d7a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  state\n",
      "0  работа полный пиддес каждый закрытие месяц сви...      0\n",
      "1          коллега сидеть рубиться долбать винд мочь      0                                                      text  state\n",
      "111921  ныне картина сжаться страх пытаться убежать ро...      1\n",
      "111922  живой придурок перестать путать нельсон мандел...      1\n",
      "                                            text  state\n",
      "0  пздца представлять козёл ахи мочь дать лёгкий      0\n",
      "1                             быть неплохой пара      1                                                     text  state\n",
      "223844  печалька увидеть катажина январь прямо плак плак      0\n",
      "223845      вчера преаод посмотреть миллыр любимый фильм      1\n"
     ]
    }
   ],
   "source": [
    "negClean = pd.DataFrame()\n",
    "posClean = pd.DataFrame()\n",
    "\n",
    "negClean['text'], negClean['state'] = tweets['negClean'], [0]*len(tweets['negClean'])\n",
    "posClean['text'], posClean['state'] = tweets['posClean'], [1]*len(tweets['posClean'])\n",
    "clean_tweets = pd.concat([negClean, posClean])\n",
    "print(clean_tweets.head(2), clean_tweets.tail(2))\n",
    "\n",
    "clean_tweets = clean_tweets.sample(frac=1).reset_index(drop=True)\n",
    "print(clean_tweets.head(2), clean_tweets.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4e56a",
   "metadata": {},
   "source": [
    "Векторизую данные с помощью CountVectorizer на триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed00aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(3, 3))\n",
    "tweetsVect = cv.fit_transform(clean_tweets['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dfa95c",
   "metadata": {},
   "source": [
    "Обучающая и тестовая выборка  \n",
    "70% обучающей и 30% тестовой  \n",
    "  \n",
    "Импортирую логистическую регрессию и обучаю на ней модель.  \n",
    "Далее строю предсказания на тестовой выборке и при сравнении с правдивыми значениями, вывожу classification_report из модуля sklearn.metrics и смотрю на macro avg f1-score что для триграмм составляет 0.47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcdadc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.97      0.69     33613\n",
      "           1       0.82      0.15      0.26     33541\n",
      "\n",
      "    accuracy                           0.56     67154\n",
      "   macro avg       0.68      0.56      0.47     67154\n",
      "weighted avg       0.68      0.56      0.47     67154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tw_train, tw_test, st_train, st_test = train_test_split(tweetsVect, clean_tweets['state'], test_size = 0.3)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter = 350)\n",
    "model.fit(tw_train, st_train)\n",
    "\n",
    "st_pred = model.predict(tw_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(st_test, st_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3403cf",
   "metadata": {},
   "source": [
    "Векторизую данные с помощью TfidfVectorizer на триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "596801b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfdv = CountVectorizer(ngram_range=(3, 3))\n",
    "tweetsVect = tfdv.fit_transform(clean_tweets['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc8fa0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fd69052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.97      0.69     33836\n",
      "           1       0.83      0.15      0.26     33318\n",
      "\n",
      "    accuracy                           0.56     67154\n",
      "   macro avg       0.68      0.56      0.48     67154\n",
      "weighted avg       0.68      0.56      0.48     67154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tw_train, tw_test, st_train, st_test = train_test_split(tweetsVect, clean_tweets['state'], test_size = 0.3)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter = 350)\n",
    "model.fit(tw_train, st_train)\n",
    "\n",
    "st_pred = model.predict(tw_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(st_test, st_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95c54f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
